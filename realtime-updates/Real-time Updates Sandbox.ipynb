{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating Parquet Files Using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import boto3\n",
    "import toolz\n",
    "import snappy\n",
    "import fastparquet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask import delayed\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "To build our \"data lake\", a couple of questions need to be answered (and accompanied by Python implementations/prototypes) related to the updating and appending to of Parquet files stored in S3. For an arbitrary dataset, there are four cases that need to be covered:\n",
    "\n",
    "1. Updating an existing Parquet file when records are added\n",
    "2. Updating an existing Parquet file when records are deleted\n",
    "3. Updating an existing Parquet file when records are altered\n",
    "\n",
    "In addition to meeting the conditions for performing the above operations, questions to considered include:\n",
    "\n",
    "1. If we use combine_first to update rows (e.g. see [here](https://citybase.atlassian.net/browse/DS-79?focusedCommentId=53549&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-53549)) in an S3+Parquet-backed Dask DataFrame, will actual computations with the result require the entire dataset? More specifically, assuming a dataset is partitioned on disk using separate files, will combine_first only generate graphs that require the files corresponding to the affected partitions? Inspect the resulting graph within DataFrame.dask.  \n",
    "2. Will an update to disk (i.e. writing back to the S3 Parquet file using to_parquet) require the entire dataset?\n",
    "\n",
    "Also, we will need to devise a means of applying [DMS-generated updates](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html).\n",
    "\n",
    "First, we'll start with a toy dataset with some updates which we will create and upload to S3 as a Parquet file. To start, we will partition into 3 partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = pd.DataFrame(data=[['Vidal', 1, 'large'],\n",
    "                              ['Brandon', 2, 'medium'],\n",
    "                              ['Alex', 3, 'large'],\n",
    "                              ['Duffy', 4, 'large'],\n",
    "                              ['Liz', 5, 'small'],\n",
    "                              ['James', 6, 'large'],\n",
    "                              ['Jack', 7, 'medium'],\n",
    "                              ['Jill', 99, 'large'],\n",
    "                              ['Donna', 9, 'large'],\n",
    "                              ['Mikey', 10, 'small']], \n",
    "                        columns=['name', 'num', 'shirt_size'], index=range(1,11))\n",
    "\n",
    "# Create Dask DataFrames for above toy data\n",
    "toy_data_dd = dd.from_pandas(toy_data, \n",
    "                             npartitions = 3)\n",
    "                                \n",
    "toy_data_dd.to_parquet('s3://cb-pdj-test/toy_data_dd.parq', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Updating Existing File With New Records\n",
    "\n",
    "When updating data to reflect addition of new records, there are two potential scenarios we might encounter.\n",
    "\n",
    "**1.** When identifying new rows of data to append to our data in S3, one approach for identifying the new rows can involve the use of a timestamp or other indicator that the record is \"new\". There may also be instances where such timestamp or indicator does not exist, and identifying the new rows necessarily involves comparing the updated data to the older data. For example, if in my older dataset I have 100 records with indices 1-100, and my updated dataset has 105 records with new records having indicies 101-105, we can determine by comparison which records are new. We assume here that old records have not been updated. My hypothesis is that determining which records are new necessarily requires a reading in of the entire dataset and a re-writing of the entire dataset.\n",
    "\n",
    "**2.** In the second scenario, our new data may come in the form of a \"stream\" or from a relational database that has a timestamp. Once those \"new\" records are identified, they can simply be appended to existing Parquet files as new partitions using `to_parquet` argument `append=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 1: Using Combine First to Update \n",
    "\n",
    "We start by setting up our \"new\" dataset, which has two new records with indicies 11 and 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data_new = pd.DataFrame(data=[['Vidal', 1, 'large'],       # Old Records\n",
    "                                 ['Brandon', 2, 'medium'],    # ...\n",
    "                                 ['Alex', 3, 'large'],        # ...\n",
    "                                 ['Duffy', 4, 'large'],       # ...\n",
    "                                 ['Liz', 5, 'small'],         # ...\n",
    "                                 ['James', 6, 'large'],       # ...\n",
    "                                 ['Jack', 7, 'medium'],       # ...\n",
    "                                 ['Jill', 99, 'large'],       # ...\n",
    "                                 ['Donna', 9, 'large'],       # ...\n",
    "                                 ['Mikey', 10, 'small'],      # ...\n",
    "                                 ['Max', 1, 'small'],         # New Record\n",
    "                                 ['Madison', 6, 'small']],    # New Record\n",
    "                        columns=['name', 'num', 'shirt_size'], \n",
    "                        index = range(1,13))\n",
    "toy_data_new_dd = dd.from_pandas(toy_data_new, chunksize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read in the Parquet file (\"old\" dataset) and use combine first to combine with the new data. We see in the graph below that all three partitions of `toy_data_dd.parq` are read and merged with the new data. In answer to question number one above, we find that, though there are only updates in this dataset to the last partition, all partitions are read when using `combine_first`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Parquet file from S3\n",
    "toy_data_rd = dd.read_parquet('s3://cb-pdj-test/toy_data_dd.parq')\n",
    "print('DIVISIONS OF OLD DATA:', toy_data_dd.divisions)\n",
    "print('DIVISIONS OF NEW DATA:', toy_data_new_dd.divisions)\n",
    "\n",
    "# Combine the old data and the new data using combine_first\n",
    "updated = toy_data_new_dd.combine_first(toy_data_rd)\n",
    "\n",
    "print('DIVISIONS OF UPDATED :', updated.divisions)\n",
    "updated.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario 2: Appending New Data\n",
    "In this scenario, I provide an example of appending rows to a dataset. Below we add two records to the Parquet file `toy_data_dd.parq`. Note that the two new records have two distinct indicies not present in the \"old\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data_append = pd.DataFrame(data=[['Max', 77, 'small'],        # New Record\n",
    "                                     ['Madison', 56, 'small']],   # New Record\n",
    "                        columns=['name', 'num', 'shirt_size'], \n",
    "                        index = [11,12])\n",
    "toy_data_append_dd = dd.from_pandas(toy_data_append, npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data_append_dd.to_parquet('s3://cb-pdj-test/toy_data_dd.parq', compression='snappy', append=True, ignore_divisions = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.read_parquet('s3://cb-pdj-test/toy_data_dd.parq').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Updating Existing File with Deleted Records\n",
    "\n",
    "For this case, we use a similar approach to that used in **Case 1, Scenario 1**. To illustrate how dropped records can be retained despite having been deleted in a newer dataset, we use `combine_first` again to get the union of two datasets. In addition to dropping two records, we also add records with indicies 11 and 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data_drop = pd.DataFrame(data=[['Vidal', 1, 'large'],       # Old Records\n",
    "                                 ['Brandon', 2, 'medium'],    # ...\n",
    "                                 ['Alex', 3, 'large'],        # ...\n",
    "                                 ['Duffy', 4, 'large'],       # ...\n",
    "                              #  ['Liz', 5, 'small'],         # DROP\n",
    "                                 ['James', 6, 'large'],       # ...\n",
    "                                 ['Jack', 7, 'medium'],       # ...\n",
    "                                 ['Jill', 99, 'large'],       # ...\n",
    "                               # ['Donna', 9, 'large'],       # DROP\n",
    "                                 ['Mikey', 10, 'small'],      # ...\n",
    "                                 ['Max', 1, 'small'],         # New Record\n",
    "                                 ['Madison', 6, 'small']],   # New Record\n",
    "                        columns=['name', 'num', 'shirt_size'], \n",
    "                        index = [1,2,3,4,6,7,8,10,11,12])\n",
    "toy_data_new_dd = dd.from_pandas(toy_data_drop, chunksize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** As noted in previous work, the result of `combine_first` in this case returns the `num` column as `float` instead of as `int`. The following cell contains code taken from Brandon's `data-frame-updates` APC workaround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/CityBaseInc/data-science-dags/blob/data-frame-updates/citybase_etl/operators/apc.py#L20\n",
    "update_dtypes = {k: getattr(toy_data_new_dd[k].dtype, 'name', v)\n",
    "                     for k, v in toy_data_new_dd.dtypes.items()}\n",
    "toy_data_new_dd = toy_data_new_dd.astype(update_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data_new_dd.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digression: Writing Only to Editied Parquet Partitions\n",
    "\n",
    "As noted in [Case 1](#Case 1: Updating Existing File With New Records), as the `to_parquet` method currently functions, there isn't a way of updating only the specific partitions that contain new or editied data. For example, if I were to append data to an existing dataset with existing indicies, an error is returned. Here is the existing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data_dd.to_parquet('./toy_data_dd.parq', compression='snappy')\n",
    "tdr = dd.read_parquet('./toy_data_dd.parq')\n",
    "tdr.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now attempt to add two new/updated records having the same indicies as two records that already exist in the dataset, at indicies 5 and 7. Note, we use `append=True`. We get an error as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data_add = pd.DataFrame(data=[['Jim', 1, 'small'],       # New Record\n",
    "                                  ['Madeline', 6, 'small']], # New Record\n",
    "                        columns=['name', 'num', 'shirt_size'], \n",
    "                        index = [5,7])\n",
    "\n",
    "# Create Dask DataFrame from toy_data_add\n",
    "tda = dd.from_pandas(toy_data_add, npartitions=1)\n",
    "\n",
    "# Append without setting ignore_divisions to False\n",
    "tda.to_parquet('./test.parq', compression='snappy', append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the above step, this time adding the argument `ignore_divisions`, the code to append the data works, and results in a new partition being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PARTITIONS BEFORE:\", dd.read_parquet('./test.parq').npartitions,\n",
    "      \"\\nDIVISIONS:\", dd.read_parquet('./test.parq').divisions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tda.to_parquet('./test.parq', compression='snappy', append=True, ignore_divisions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PARTITIONS AFTER:\", dd.read_parquet('./test.parq').npartitions,\n",
    "      \"\\nDIVISIONS:\", dd.read_parquet('./test.parq').divisions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with a new partition being created is that now we have a new partition that does not follow the ordered convention of the other partitions. Furthermore, we lose the divisions indicies in the metadata. Also, now there are two records with the index 5 and index 7. We could technically proceed as is and handle duplicates in the future upon reading in the data, but then we get run into the issue of re-reading and re-writing the entire dataset upon deletion and correction of the indicies all over again.\n",
    "\n",
    "A desirable feature for updating Parquet files with Dask would be the ability for Dask to update records for indicies that already exist by:\n",
    "1. Read the Parquet metadata to determine where those indicies are\n",
    "2. Read in the data for only those partitions\n",
    "3. Reconcile the changes in within the Pandas dataframe\n",
    "4. Re-write the partition\n",
    "5. Update the meta data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Dask and Fastparquet Source Code\n",
    "\n",
    "Next, I'll examine the source code for both Dask and Fastparquet to determine where the necessary changes to either may need to be made. The following questions should be addressed:\n",
    "\n",
    "1. When reading in a Parquet file, where does the meta data get read and how is that meta data represented and used?\n",
    "2. When appends to Parquet files are made, where in the meta data are there updates and how are they handled?\n",
    "\n",
    "#### Reading in Parquet Files and Examining Metadata\n",
    "\n",
    "As can be seen in the [Dask source code](https://github.com/dask/dask/blob/5255092ecb98858451d35efaedb3ae03036870b1/dask/dataframe/io/parquet.py#L177), parquet files are read using the [`ParquetFile`](https://github.com/dask/fastparquet/blob/master/fastparquet/api.py#L25) class from the `fastparquet` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = fastparquet.api.ParquetFile('./toy_data_dd.parq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file structure for each `.parq` file is as follows:\n",
    "- toy_data_dd.parq\n",
    "    - _common_metadata\n",
    "    - _metadata\n",
    "    - **part.0.parquet** | type `fastparquet.parquet_trift.parquet.ttypes.RowGroup`\n",
    "        - column1, 'index' | type `fastparquet.parquet_thrift.parquet.ttypes.ColumnChunk`  \n",
    "          .  \n",
    "          .  \n",
    "        - column4, 'shirt_size' | type `fastparquet.parquet_thrift.parquet.ttypes.ColumnChunk`  \n",
    "        <br>\n",
    "    - **part.1.parquet** | type `fastparquet.parquet_trift.parquet.ttypes.RowGroup`  \n",
    "        - column1, 'index' | type `fastparquet.parquet_thrift.parquet.ttypes.ColumnChunk`  \n",
    "          .  \n",
    "          . \n",
    "        - column4, 'shirt_size' | type `fastparquet.parquet_thrift.parquet.ttypes.ColumnChunk`  \n",
    "        <br>\n",
    "    - **part.2.parquet** | type `fastparquet.parquet_trift.parquet.ttypes.RowGroup`  \n",
    "        - column1, 'index' | type `fastparquet.parquet_thrift.parquet.ttypes.ColumnChunk`  \n",
    "          .  \n",
    "          .  \n",
    "        - column4, 'shirt_size' | type `fastparquet.parquet_thrift.parquet.ttypes.ColumnChunk`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1 = f3.row_groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = part1.columns[0]\n",
    "col1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding: Partitions Overwritable Without Updating Metadata - Kinda\n",
    "It's possible to write two distinct `.parq` files, swap out a `part.i.parquet` file from one `.parq` folder to another, and sucessfully read in the data with the changes reflected. However, it does have limitations. Anytime the type of a value is changed (ex. changing a string for an integer), it suddently fails to read because the file's data does not match the metadata, which remained unchanged when the file swap was made. Here is an example where we make a change in line 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swap = pd.DataFrame(data=[['Vidal', 1, 'large'],\n",
    "                          ['Jessica', 2, 'xlarge'],\n",
    "                          ['Jose', 3, 'small'],\n",
    "                          ['Brandon', 4, 'medium']], \n",
    "                        columns=['name', 'num', 'shirt_size'], index=[1,2,3,4])\n",
    "\n",
    "swap2 = pd.DataFrame(data=[['Vidal', 1, 'large'],\n",
    "                           ['Jessica', 2, 'xlarge'],\n",
    "                           ['Jose', 3, 'small'],\n",
    "                           ['nodnarB', 455, 'medium']], #****** CHANGE HERE *******#\n",
    "                        columns=['name', 'num', 'shirt_size'], index=[1,2,3,4])\n",
    "# Create Dask DataFrame from Pandas\n",
    "swap_dd = dd.from_pandas(swap, npartitions=2)\n",
    "swap_dd2 = dd.from_pandas(swap2, npartitions=2)\n",
    "\n",
    "# Write both Dask DataFrames to Parquet\n",
    "swap_dd.to_parquet('./swap_test1.parq', compression='snappy')\n",
    "swap_dd2.to_parquet('./swap_test2.parq', compression='snappy')\n",
    "\n",
    "print('BEFORE SWAP:---------\\n', dd.read_parquet('./swap_test1.parq/').compute(), '\\n')\n",
    "\n",
    "!mv ./swap_test2.parq/part.1.parquet ./swap_test1.parq/\n",
    "\n",
    "print('AFTER SWAP:----------\\n', dd.read_parquet('./swap_test1.parq/').compute())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that, though we are reading in the same `.parq` file, we see different values for `name` and `num` at index 4 because I swapped the `part.1.parquet`. However, this swapping of files fails when we make the following types of changes, as done in line 10 (all else is the same):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swap = pd.DataFrame(data=[['Vidal', 1, 'large'],\n",
    "                          ['Jessica', 2, 'xlarge'],\n",
    "                          ['Jose', 3, 'small'],\n",
    "                          ['Brandon', 4, 'medium']], \n",
    "                        columns=['name', 'num', 'shirt_size'], index=[1,2,3,4])\n",
    "\n",
    "swap2 = pd.DataFrame(data=[['Vidal', 1, 'large'],\n",
    "                           ['Jessica', 2, 'xlarge'],\n",
    "                           ['Jose', 3, 'small'],\n",
    "                           ['Brandon', 'hi', 'medium']], #********* CHANGES HERE **********#\n",
    "                        columns=['name', 'num', 'shirt_size'], index=[1,2,3,4])\n",
    "# Create Dask DataFrame from Pandas\n",
    "swap_dd = dd.from_pandas(swap, npartitions=2)\n",
    "swap_dd2 = dd.from_pandas(swap2, npartitions=2)\n",
    "\n",
    "# Write both Dask DataFrames to Parquet\n",
    "swap_dd.to_parquet('./swap_test1.parq', compression='snappy')\n",
    "swap_dd2.to_parquet('./swap_test2.parq', compression='snappy')\n",
    "\n",
    "print('BEFORE SWAP:---------\\n', dd.read_parquet('./swap_test1.parq/').compute(), '\\n')\n",
    "\n",
    "!mv ./swap_test2.parq/part.1.parquet ./swap_test1.parq/\n",
    "\n",
    "print('AFTER SWAP:----------\\n', dd.read_parquet('./swap_test1.parq/').compute())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, copying partition files from one `.parq` file to another, though it may suggest that we can simply overwrite a single partition without needing to update the metadata, it only works for some situations, and is ultimately **NOT** a solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring How Append Works\n",
    "The answer to making updates to partitions while making edits to existing metadata may lie in the code `to_parquet` when for `append=True` `ignore_divisions=False`. First, I want to test if using `append=True` rewrites all of the `part.i.parquet` files by trying a toy example and checking the Date Created timestamps for the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=[['Vidal', 1, 'large'],\n",
    "                          ['Jessica', 2, 'xlarge'],\n",
    "                          ['Jose', 3, 'small'],\n",
    "                          ['Brandon', 4, 'medium']], \n",
    "                        columns=['name', 'num', 'shirt_size'], index=[1,2,3,4])\n",
    "\n",
    "appe = pd.DataFrame(data=[['Jose', 3, 'small'],      #********* APPEND **********#\n",
    "                          ['Brandon', 7, 'medium']], #********* APPEND **********#\n",
    "                        columns=['name', 'num', 'shirt_size'], index=[5,6])\n",
    "\n",
    "data = dd.from_pandas(data, npartitions= 2)\n",
    "appe = dd.from_pandas(appe, npartitions= 1)\n",
    "data.to_parquet(\"s3://cb-pdj-test/append_test.parq\", compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appe.to_parquet(\"s3://cb-pdj-test/append_test.parq\", compression='snappy', append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis Confirmed**: When using `to_parquet` with `append=True`, only the new partition is written and the metadata is created anew, leaving the other partitions untouched. So, in order to figure out how to update specific parititions, I need to identify:\n",
    "1. How the new partition is written\n",
    "2. How the metadata is updated\n",
    "3. Determine if it's possible to change a partition in the middle (if there are three partitions, with file names `part.0.parquet`, `part.1.parquet`, `part.2.parquet` and `part.3.parquet`, is it possible to overwrite `part.2.parquet`, update the metadata accordingly, and still read in the data after the rewriting has happend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to 1:** Just as every other partition, use [`_write_partition_fastparquet`](https://github.com/dask/dask/blob/master/dask/dataframe/io/parquet.py#L524) as shown [here](https://github.com/dask/dask/blob/master/dask/dataframe/io/parquet.py#L614) to write the partition.\n",
    "\n",
    "**Answer to 2:** Metadata is written using [`_write_metadata`](https://github.com/dask/dask/blob/master/dask/dataframe/io/parquet.py#L621) and takes only the new dataframes + filenames along with the metadata from original parquet file as inputs.\n",
    "\n",
    "**Answer to 3:** It's possible! Use the `.append` and `.pop` methods for `list` of RowObjects in file metadata.  \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Getting Closer: Steps Needed to Update Only Paritions with Updated Data\n",
    "\n",
    "In order update a parquet file only in the affected partitions, we must:  \n",
    "1. Use the index value of each updated record to identify which paritions need updating\n",
    "2. Read in the data in those partitions and reconcile data changes\n",
    "3. Write the new parition using the filename of the old partition\n",
    "4. Write the new metadata\n",
    "    - Drop the partition/row_group from the `fmd`(file metadata) object\n",
    "    - Append the new `row_group` to the `fmd`\n",
    "    - Rewrite the metadata files to reflect changes\n",
    "\n",
    "I will demonstrate updates to an existing parquet file using this toy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delayed('_write_metadata-3945ec01-2a70-47d2-b364-59d6985ebd81')\n",
      "CPU times: user 20.9 ms, sys: 5.09 ms, total: 26 ms\n",
      "Wall time: 22.6 ms\n"
     ]
    }
   ],
   "source": [
    "old_data = pd.DataFrame(data=[['Vidal', 1, 'large'],\n",
    "                              ['Brandon', 2, 'medium'],\n",
    "                              ['Alex', 3, 'large'],\n",
    "                              ['Duffy', 4, 'large'],\n",
    "                              ['Liz', 5, 'small'],\n",
    "                              ['James', 6, 'large'],\n",
    "                              ['Jack', 7, 'medium'],\n",
    "                              ['Jill', 99, 'large'],\n",
    "                              ['Donna', 9, 'large'],\n",
    "                              ['Mikey', 10, 'small']], \n",
    "                        columns=['name', 'num', 'shirt_size'], index=range(1,11))\n",
    "\n",
    "# Create Dask DataFrames for above toy data\n",
    "old_data_dd = dd.from_pandas(old_data, \n",
    "                             npartitions = 3)\n",
    "                                \n",
    "%time old_data_dd.to_parquet('./old_data_dd.parq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make updates to the above dataset at indicies 1, 9, and 10 using this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame(data=[['Vidal', 1, '2xlarge'],\n",
    "                              ['Liz', 5, 'small'],\n",
    "                              ['Joe', 8, 'small']], \n",
    "                        columns=['name', 'num', 'shirt_size'], index=[1,9,10])\n",
    "\n",
    "# Create Dask DataFrames for above toy data\n",
    "new_data_dd = dd.from_pandas(new_data, \n",
    "                             npartitions = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min': [1, 5, 9], 'max': [4, 8, 10]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDICIES OF CHANGED DATA: [1, 9, 10]\n",
      "INDICIES IN EACH PART OF OLD: {0: range(1, 5), 1: range(5, 9), 2: range(9, 11)}\n",
      "PARTITIONS AFFECTED: {0: {1}, 1: set(), 2: {9, 10}}\n"
     ]
    }
   ],
   "source": [
    "# 1a. Grab the index values of the records being updated\n",
    "new_indicies = list(new_data_dd.compute().index)\n",
    "print(\"INDICIES OF CHANGED DATA:\", new_indicies)\n",
    "\n",
    "# 1b. Use those index values to identify the partitions being affected\n",
    "\n",
    "## Read in the old datasets metadata\n",
    "old = fastparquet.api.ParquetFile('./old_data_dd.parq')\n",
    "fmd = old.fmd\n",
    "\n",
    "## Get index value ranges for each of the partitions in the old dataset\n",
    "ind = fastparquet.api.sorted_partitioned_columns(old)\n",
    "indicies = ind['index']\n",
    "old_indicies = {}\n",
    "for part_num, index_pair in enumerate(zip(indicies['min'], indicies['max'])):\n",
    "    old_indicies[part_num] = range(index_pair[0], index_pair[1] + 1)\n",
    "    \n",
    "print(\"INDICIES IN EACH PART OF OLD:\", old_indicies)\n",
    "\n",
    "part_index_map = {}\n",
    "\n",
    "## Create a dictionary which maps indicies of new data to the partitions which hold the records\n",
    "## TODO: Look for computationally simpler implementation of this\n",
    "for oi in old_indicies:\n",
    "    part_index_map[oi] = set()\n",
    "    for i in new_indicies:\n",
    "        if i in set(old_indicies[oi]):\n",
    "            part_index_map[oi].add(i)\n",
    "            \n",
    "print(\"PARTITIONS AFFECTED:\", part_index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Read in the data in those partitions and reconcile differences\n",
    "Using the mapping of paritions to indicies from above, I loop through the mappings to:\n",
    "- Subset the new data by the indicies according to each partition\n",
    "- Read in the old data in each partition\n",
    "- Use combine_first to update the data\n",
    "- Write the new partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition: 0 \n",
      "File Name:\n",
      " part.0.parquet \n",
      "Old Data:\n",
      " Dask DataFrame Structure:\n",
      "                 name    num shirt_size\n",
      "npartitions=1                          \n",
      "1              object  int64     object\n",
      "4                 ...    ...        ...\n",
      "Dask Name: read-parquet, 1 tasks \n",
      "New Data:\n",
      "    index     name  num shirt_size\n",
      "0      1    Vidal  1.0    2xlarge\n",
      "1      2  Brandon  2.0     medium\n",
      "2      3     Alex  3.0      large\n",
      "3      4    Duffy  4.0      large \n",
      "\n",
      "\n",
      "\n",
      "Partition: 2 \n",
      "File Name:\n",
      " part.2.parquet \n",
      "Old Data:\n",
      " Dask DataFrame Structure:\n",
      "                 name    num shirt_size\n",
      "npartitions=1                          \n",
      "9              object  int64     object\n",
      "10                ...    ...        ...\n",
      "Dask Name: read-parquet, 1 tasks \n",
      "New Data:\n",
      "    index name  num shirt_size\n",
      "0      9  Liz    5      small\n",
      "1     10  Joe    8      small \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dask.dataframe.io.parquet import _write_partition_fastparquet\n",
    "from dask.bytes.core import get_fs_token_paths\n",
    "path = './old_data_dd.parq'\n",
    "partition_on = None\n",
    "compression = None\n",
    "storage_options = None\n",
    "fs, fs_token, _ = get_fs_token_paths(path, mode='wb',\n",
    "                                         storage_options=storage_options)\n",
    "sep = fs.sep\n",
    "\n",
    "writes = []\n",
    "filenames = []\n",
    "\n",
    "for part in part_index_map:\n",
    "    if len(part_index_map[part]) != 0:\n",
    "        indicies = list(part_index_map[part])\n",
    "        new_part = new_data_dd.loc[indicies]\n",
    "        \n",
    "        filename = 'part.%i.parquet' % part\n",
    "        old_part = dd.read_parquet(fs.sep.join([path, filename]))\n",
    "        old = fastparquet.api.ParquetFile('./old_data_dd.parq/' + filename)\n",
    "        fmd = copy.copy(old.fmd)\n",
    "        new_part = new_part.combine_first(old_part)\n",
    "        new_part = new_part.compute().reset_index() \n",
    "\n",
    "        write = _write_partition_fastparquet(new_part, fs, path, filename, fmd, \n",
    "                                     compression, partition_on)\n",
    "        writes.append(write)\n",
    "        filenames.append(filename)\n",
    "        print(\n",
    "            \"Partition:\", part,\n",
    "            \"\\nFile Name:\\n\", filename,\n",
    "            \"\\nOld Data:\\n\", old_part,\n",
    "            \"\\nNew Data:\\n\", new_part,\n",
    "            \"\\n\\n\\n\"\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Write the new parition using the filename of the old partition**\n",
    "\n",
    "This step was completed in the previous cell. To confirm that the partitions wrote successfully, we can read them in individually. We can see the changes reflected below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>num</th>\n",
       "      <th>shirt_size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vidal</td>\n",
       "      <td>1</td>\n",
       "      <td>2xlarge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brandon</td>\n",
       "      <td>2</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alex</td>\n",
       "      <td>3</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Duffy</td>\n",
       "      <td>4</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name  num shirt_size\n",
       "index                         \n",
       "1        Vidal    1    2xlarge\n",
       "2      Brandon    2     medium\n",
       "3         Alex    3      large\n",
       "4        Duffy    4      large"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.read_parquet('./old_data_dd.parq/part.0.parquet').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>num</th>\n",
       "      <th>shirt_size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Liz</td>\n",
       "      <td>5</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Joe</td>\n",
       "      <td>8</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  num shirt_size\n",
       "index                     \n",
       "9      Liz    5      small\n",
       "10     Joe    8      small"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.read_parquet('./old_data_dd.parq/part.2.parquet').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Write the new metadata**  \n",
    "   - Drop the partition/row_group from the `fmd`(file metadata) object\n",
    "   - Append the new `row_group` to the `fmd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.dataframe.io.parquet import _write_metadata\n",
    "old = fastparquet.api.ParquetFile('./old_data_dd.parq')\n",
    "fmd = copy.copy(old.fmd)\n",
    "\n",
    "partitions = set(range(len(fmd.row_groups)-1))\n",
    "parts = set([0,2])\n",
    "parts = partitions - parts\n",
    "\n",
    "fmd.row_groups = [fmd.row_groups[i] for i in parts]\n",
    "\n",
    "\n",
    "## Write new metadata\n",
    "_write_metadata(writes, filenames, fmd, path, fs, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_dd = new_data_dd.reset_index()\n",
    "new_data_dd.divisions = (1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "We can see the changes to the data reflected below. This example demonstrates the ability to modify parquet files at the partition-level in cases where records located in only a few parititons are updated. We are able to update the data without having to read in the entire dataset and without rewriting the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>num</th>\n",
       "      <th>shirt_size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Liz</td>\n",
       "      <td>5</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>James</td>\n",
       "      <td>6</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jack</td>\n",
       "      <td>7</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jill</td>\n",
       "      <td>99</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vidal</td>\n",
       "      <td>1</td>\n",
       "      <td>2xlarge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brandon</td>\n",
       "      <td>2</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alex</td>\n",
       "      <td>3</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Duffy</td>\n",
       "      <td>4</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Liz</td>\n",
       "      <td>5</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Joe</td>\n",
       "      <td>8</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name  num shirt_size\n",
       "index                         \n",
       "5          Liz    5      small\n",
       "6        James    6      large\n",
       "7         Jack    7     medium\n",
       "8         Jill   99      large\n",
       "1        Vidal    1    2xlarge\n",
       "2      Brandon    2     medium\n",
       "3         Alex    3      large\n",
       "4        Duffy    4      large\n",
       "9          Liz    5      small\n",
       "10         Joe    8      small"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.read_parquet('./old_data_dd.parq').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delayed('_write_metadata-530f6df0-6855-4b16-9d95-159231fed9b5')\n",
      "CPU times: user 35.6 ms, sys: 4.37 ms, total: 40 ms\n",
      "Wall time: 37.4 ms\n"
     ]
    }
   ],
   "source": [
    "%time new_data_dd.to_parquet('./old_data_dd.parq', replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtu",
   "language": "python",
   "name": "rtu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
